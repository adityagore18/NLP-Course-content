NLP pipe line 
A)Data Acquisition
Way
1>Public data
2> json & sqL
     json
     import pandas as pd
     df=pd.read_json("json_file")
     sql
      1>conn object
      2>pd.read_sql_query("select * from tbl",conn_object) 
      How create connection ???
      Ans: pymysql(module import)
              import pymysql
              conn = pymysql.connect( host="localhost" ,user="root",password="sql_pass",database="db_nm")
       How read data ?
              import pandas as pd
              df=pd.read_sql_query("select * from tbl",conn)
              df.to_csv("file_nm")
     
3> data through api
      import pandas as pd
      import requests (module)
      response=requests.get("api url")
      response.json()
     df=pd.DataFrame(response.json())
     df.to_csv("file_nm")

4>WEB Scrapping
     import requests
     from bs4 import BeautifulSoup
     import requests (module)
     response=requests.get("web url")
     response.status_code (200 ,Ok)
     response.text
     bs=BeautifulSoup(response.text,"lxml")
     bs.prettify()
     bs.find_all("tag_name",class="class_name") =>return list having tag object all having same tag_name ather class name also same mention in find all 
     bs.find("tag_name",class="class_name")  sigle tag object
     Now we want only text ,How we extract ????
     => object.text  =>get only text not any tag
     => bs.find("tag_name",class="class_name").text
     =>bs.find_all("tag_name",class="class_name")[indx].text

B)Text preparation
    1>Text Cleanup(Lowercasing,Remove html tag,Remove eomji,spelling correction)
    2>Basic Text preprocessing (Remove Stop word,Remove Punctuation ,Lemmatization,Stemming,Tokenization)
    3>Advanced preprocessing (POS ,Parsing )
    1> Text Cleanup
     a) Lowercasing=> df['feature']=df['feature'].str.lower()
     b)Remove Html tag (use regex)/URL removing
       Html_regex="<.*?>"
      Url_regex= 'https?://\S+|www\. \S+'
       import re(Python module)
       def rmv_html_tag/url_removing(text):
             pattern = re.compile("<.*?>" /  'https?://\S+|www\. \S+' )
             return pattern.sub("",text)
       df['feature']=df['feature'].apply(rmv_html_tag/url_removing)
     c)Remove emoji
        import emoji (Python module)
        emoji_txt="Python is on ðŸ”¥"
        emoji.demojize(emoji_txt)
     d)Spelling correction
         from textblob import TextBlob
         txt = TextBlob("wrong spelled txt")
         txt.correct().string =>correct spelled text return
       2>Basic Text preprocessing
           1> Remove Stop word
             (Text processing imp library is nltk)
              from nltk.corpus import stopwords
              english stop word you want  => stopwords.words("english")
              spanish stop word you want  => stopwords.words("spanish")
                def rmv_stopword(str):        
                      new_text=[]
                      for word in str.split():
                           if word in stopwords.words('english'):
                    #        print(word)
                              new_text.append('')
                          else:
                              new_text.append(word)
                      x=new_text[:]
                      new_text.clear()
                      return " ".join(x)
             2>Remove Punctuation
                  import string
                  exclude=string.punctuation=>'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
                  def remv_punc(text):
    			return text.translate(str.maketrans('','',exclude))
     	    3>Stemming =>stemming geted word sometime not a english word also
                from nltk.stem import PorterStemmer
                stemmer = PorterStemmer()
                def stemming(txt):
			str="";
                        for word in txt.split():
                            str=" "+stemmer.stem(word)
                        return str.lstrip()
                *short way same thing
                ps=PorterStemmer()
		def stem_word(text):
   			 return " ".join([ps.stem(word) for word in text.split()])
              4>Lemmatization=>wordnet lamitizer used it is lexical dictionary to find lemma of word is search in wordnet
                   from nltk.stem import WordNetLemmatizer 
                   lemmatizer=WordNetLemmatizer()
		   sample="He was running and eating at same time"
	           for word in word_tokenize(sample):
   		   print(word,lemmatizer.lemmatize(word,pos="v"))
C)Feature engineering(Text Vectorization)
     1>ONE hot encoding
     2>Bag Of Word
     3>n gram
     4>TF-IDF
     5>word2vec (pre trained model developed by google engineers 2013) (Deep learning algo)
     1>ONE Hot Encoding
      2>Bag Of Word =>ngram_range=Here this value unity by default
       from sklearn.feature_extraction.text import CountVectorizer
       cv=CountVectorizer()
       bow=cv.fit_transform(df['text'])
       cv.vocabulary_
       bow[0].toarray()
      3> n gram
       from sklearn.feature_extraction.text import CountVectorizer
       cv=CountVectorizer(ngram_range=(2,2))
       bow=cv.fit_transform(df['text']) 
     4>TF-IDF
          from sklearn.feature_extraction.text import TfidfVectorizer
          tfidf=TfidfVectorizer()
          tfidf.fit_transform(df['text']).toarray()
          print(tfidf.idf_)
          print(tfidf.vocabulary_)
D)Modeling
    approaches => Heuristic ,ML, Deep Learning based
    1>modelling
    2> evalution
    1>
    Approach used  :- ML Based
    ML algo used
    1> Navie Bayes=> GussianNB, MultinomialNB
    2> Logistic regression
    3> Random Forest
    4> SVM
    STEPS FOR CREATING MODEL
    import model
    data split (train_test_split model)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    Model object (obj=Model())
    Model train (obj.fit(X_train,y_train))
    Test ( obj.predict(X_test) )
    importing  things you must Know
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from sklearn.naive_bayes import MultinomialNB
    


      
       

    